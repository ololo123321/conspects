\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{derivative}
\usepackage{tikz}
\usepackage{xcolor}

\begin{document}

\title{Derivative of BatchNorm and LayerNorm operation w.r.t $X$}
\author{Kadulin V.}
%\date{}
\maketitle

Let $Y = \gamma \hat{X} + \beta$, where 
$\hat{X} \in \mathbb{R}^{N \times D}$, 
$\gamma \in \mathbb{R}^D$, 
$\beta \in \mathbb{R}^D$, 
$\hat{X} = \frac{X - \mu}{\sigma}$.
Let's start from BatchNorm case, where  
$\mu = \frac{1}{N} \sum_{i=1}^{N} x_{i}$,
$v = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2$,
$\sigma = \sqrt{v + \epsilon}$,
$\epsilon \in \mathbb{R}$ is a small scalar to omit zero division. 

Let $f: \mathbb{R}^{N \times D} \rightarrow \mathbb{R}$ is a differentiable scalar function. We also know $\pdv{f}{Y}$.
Derive $\pdv{f}{X}$.

Notice that
\begin{itemize}
\item $y_i$ depends on $x_j$, where $1 \le j \le N$
\item there is only per-column dependency: $y_{ij}$ depends on $x_{kj}$, where $1 \le k \le N$
\end{itemize}

Consider first column of $X$ as $x$.

\begin{equation*}
\begin{split}
y = \gamma \hat{x} + \beta \\
y, x, \hat{x} \in \mathbb{R}^N \\
\gamma, \beta, \mu, v, \sigma \in \mathbb{R}
\end{split}
\end{equation*}

Let's consider $j$-th element of $x$ as $x_j$.

\[
	\pdv{f}{x_j} = 
	\sum_{i=1}^{N} \pdv{f}{y_i} \pdv{y_i}{\hat{x_i}} \pdv{\hat{x_i}}{x_j}
\]

\newpage

Let's derive expressions for building blocks:

$\pdv{y}{\hat{x_i}} = \gamma$ 

$\pdv{\hat{x_i}}{x_j} = 
\pdv{(x_i - \mu)}{x_j} (v + \epsilon)^{-\frac{1}{2}} + (x_i - \mu) (-\frac{1}{2}) (v + \epsilon)^{-\frac{3}{2}} \pdv{(v + \epsilon)}{x_j}$

$\pdv{\mu}{x_j} = \frac{1}{N}$

$
\pdv{x_i}{x_j} = 
\begin{cases}
1 &\text{$i = j$}\\
0 &\text{$i \ne j$}
\end{cases}
$

Let's derive $\pdv{v}{x_j}$.

\section{Forward-mode differentiation}

Consider $i$-th component of sum in $v$ as $y_i$.

$
\pdv{y_i}{x_j} = 
\begin{cases}
2 (x_i - \mu) (1 - \frac{1}{N}) = -\frac{2}{N} (x_i - \mu) + 2(x_i - \mu) & \text{$i = j$} \\
-\frac{2}{N} (x_i - \mu) & \text{$i \ne j$}
\end{cases}
$ 

So, $\pdv{v}{x_j} = \frac{1}{N} ( -\frac{2}{N} \sum_{i=1}^{N}(x_i - \mu) + 2(x_j - \mu))$.

Note, that $\sum_{i=1}^{N}(x_i - \mu) = \sum_{i=1}^{N} x_i - N \mu = N \mu - N \mu = 0$.

So, $\pdv{v}{x_j} = \frac{2}{N} (x_j - \mu)$.

Now we can expand $\pdv{\hat{x_i}}{x_j}$. Let's do it in case $i \ne j$:

\begin{gather*}
\begin{split}
\pdv{\hat{x_i}}{x_j} =
-\frac{1}{N} (v + \epsilon)^{-\frac{1}{2}} + (x_i - \mu) (-\frac{1}{2}) (v + \epsilon)^{-\frac{3}{2}} \frac{2}{N}(x_j - \mu) = \\
-\frac{1}{N} (v + \epsilon)^{-\frac{1}{2}} (1 + (x_i - \mu) (x_j - \mu) (v + \epsilon)^{-1})
\end{split}
\end{gather*}

Note that the only difference of case $i = j$ is the first multiplier of the first term: it's $1 - \frac{1}{N}$ instead of $-\frac{1}{N}$.

Now we have all to derive $\pdv{f}{x_j}$:

\begin{gather*}
\pdv{f}{x_j} = 
\sum_{i=1}^{N} \pdv{f}{y_i} \pdv{y_i}{\hat{x_i}} \pdv{\hat{x_i}}{x_j} = 
\gamma \sum_{i=1}^{N} \pdv{f}{y_i} \pdv{\hat{x_i}}{x_j} = \\
\gamma \left( \sum_{i=1}^{N} -\frac{1}{N} (v + \epsilon)^{-\frac{1}{2}} (1 + (x_i - \mu) (x_j - \mu) (v + \epsilon)^{-1}) \pdv{f}{y_i} + (v + \epsilon)^{-\frac{1}{2}} \pdv{f}{y_j} \right) = \\
\gamma (v + \epsilon)^{-\frac{1}{2}} \left( -\frac{1}{N} \sum_{i=1}^{N} (1 + (x_i - \mu) (x_j - \mu) (v + \epsilon)^{-1}) \pdv{f}{y_i} + \pdv{f}{y_j} \right) = \\
\gamma (v + \epsilon)^{-\frac{1}{2}} \left( -\frac{1}{N} \sum_{i=1}^{N} \pdv{f}{y_i} - (x_j - \mu) (v + \epsilon)^{-1} \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu) \pdv{f}{y_i} + \pdv{f}{y_j} \right)
\end{gather*}

Knowing that, it's easy to write expression in matrix form:

\[
	\pdv{f}{X} = \gamma (v + \epsilon)^{-\frac{1}{2}} \left( -\frac{1}{N} \sum_{i=1}^{N} \pdv{f}{y_i} - (X - \mu) (v + \epsilon)^{-1} \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu) \pdv{f}{y_i} + \pdv{f}{Y} \right) 
\]

Here $x, y, v, \mu, \gamma \in \mathbb{R}^D$.

\section{Backward-mode differentiation}

Let's draw a computation graph:

\begin{tikzpicture}[node distance={25mm}, main/.style = {draw, circle}]
\node[main] (1) {$x_j$};
\node[main] (2) [above right of=1] {$\mu$};
\node[main] (3) [below right of=1] {$v$};
\node[main] (4) [below right of=2] {$\hat{x_i}$};
\node[main] (5) [below right of=4] {$\gamma$};
\node[main] (6) [above right of=5] {$y_i$};
\node[main] (7) [right of=6] {$f$};
\draw[->] (1) -- (2);
\draw[->] (1) -- (3);
\draw[->] (1) -- (4);
\draw[->] (2) -- (3);
\draw[->] (2) -- (4);
\draw[->] (3) -- (4);
\draw[->] (4) -- (6);
\draw[->] (5) -- (6);
\draw[->] (6) -- (7);
\end{tikzpicture}

Note that there are three paths from $x_j$ and two paths from $\mu$. So, derivative expressions through these nodes will consist of three and two terms respectively:

\[
	\pdv{f}{x_j} = 
	\sum_{i=1}^{N}
	\pdv{f}{y_i}
	\pdv{y_i}{\hat{x_i}} 
	\left( 
		\pdv{\hat{x_i}}{x_j} + 
		\pdv{\hat{x_i}}{v}\pdv{v}{x_j} + 
		\left( 
			\pdv{\hat{x_i}}{\mu} + 
			\pdv{\hat{x_i}}{v} \pdv{v}{\mu} 
		\right) 
		\pdv{\mu}{x_j} 
	\right) 
\]

$\pdv{y_i}{\hat{x_i}} = \gamma$

$
\pdv{\hat{x_i}}{x_j} = 
\begin{cases}
1 & \text{$i = j$} \\
0 & \text{$i \ne j$}
\end{cases}
$

$\pdv{\hat{x_i}}{v} = (x_i - \mu) (- \frac{1}{2}) (v + \epsilon)^{-\frac{3}{2}}$

$\pdv{v}{x_j} = \frac{2}{N} (x_j - \mu)$

$\pdv{\hat{x_i}}{\mu} = - (v + \epsilon)^{-\frac{1}{2}}$

$\pdv{v}{\mu} = - \frac{2}{N} \sum_{i=1}^{N} (x_i - \mu) = 0$

$\pdv{\mu}{x_j} = \frac{1}{N}$

Now let's derive $\pdv{f}{x_j}$:

\begin{gather*}
\pdv{f}{x_j} = 
	\gamma 
	\left(
		\sum_{i=1}^{N} \pdv{f}{y_i} 
			\left( 
				- (x_i - \mu) (v + \epsilon)^{-\frac{3}{2}} \frac{1}{N} (x_j - \mu) - \frac{1}{N} (v + \epsilon)^{-\frac{1}{2}}
			\right) 
			+ (v + \epsilon)^{-\frac{1}{2}} \pdv{f}{y_j}
	\right) = \\
	\gamma (v + \epsilon)^{-\frac{1}{2}} 
	\left(	
		- \frac{1}{N} \sum_{i=1}^{N} \pdv{f}{y_i}
		- (x_j - \mu) (v + \epsilon)^{-1} \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu) \pdv{f}{y_i} 
		+ \pdv{f}{y_j}
	\right)
\end{gather*}

Note that result is the same as in previous approach.

\section{LayerNorm}

This operation differs from BatchNorm in a single aspect: $\mu$ and $v$ are calculated by columns instead of rows. This implies the following changes:

$\pdv{y_i}{\hat{x_i}} = \gamma_i$

$\pdv{\mu}{x_j} = \frac{1}{D}$


So, the equation for LayerNorm operation looks as follows:

\[	
	\pdv{f}{X} = 
	(v + \epsilon)^{-\frac{1}{2}} 
	\left(	
		- \frac{1}{\begingroup\color{red} D \endgroup} 
		\sum_{i=1}^{\begingroup\color{red} D \endgroup} 
		\begingroup\color{red} \gamma_i \endgroup
		\pdv{f}{y_i}
		- (X - \mu) (v + \epsilon)^{-1} 
		\frac{1}{\begingroup\color{red} D \endgroup} 
		\sum_{i=1}^{\begingroup\color{red} D \endgroup} 
		\begingroup\color{red} \gamma_i \endgroup
		(x_i - \mu) \pdv{f}{y_i} 
		+ \begingroup\color{red} \gamma \endgroup \pdv{f}{Y}
	\right)
\]

\section{Other normalization methods}

Now we know two normalization methods for 2-D case: BatchNorm and LayerNorm. But there are some details in case of images, which have a spatial structure. Consider a batch of images of shape $\mathbb{R}^{N \times C \times H \times W}$, where $N$ - batch size, $C$ - number of channels (or feature maps), $H$ - height, $W$ - width.

Here is a comparison of different normalization methods:

\begin{center}
\begin{tabular}{||l c c c||} 
 \hline
 name & per & over & norm \\ [0.5ex] 
 \hline
 \hline 
 
 BatchNorm & D & N & D \\ 
 \hline
 LayerNorm & N & D & D \\
 \hline
 Spatial BatchNorm & C & N, H, W & C \\
 \hline
 GroupNorm & N, G & C / G, H, W & C \\
 \hline
 InstanceNorm & N, C & H, W & C \\ [1ex] 
 \hline
\end{tabular}
\end{center}

\begin{itemize}
\item \textit{per} - independent elements
\item \textit{over} - computing moments
\item \textit{norm} - scale and shift axis
\end{itemize}

Some notes about each method:

\begin{itemize}
\item BatchNorm -- normalizes each feature independently. Quality of moments depends of batch size - higher is better.
\item LayerNorm -- computes statistics accross whole features. This fact makes it more preferable in case of small batches. Assumes equal contribution of each feature.
\item Spatial BatchNorm -- normalizes each feature map independently. Makes statistics consistent accross different images and image regions.
\item GroupNorm -- LayerNorm analogue for images, where aggregation is also done per channel groups: hypothesis is that feature maps are grouped by some factors like frequency, shapes, illumination, textures (examples from original paper). If so, each group might have different moments. Parametrized by $G$ - number of groups of feature maps.
\item InstanceNorm -- special case of GroupNorm where $G = 1$.
\end{itemize}

\end{document}