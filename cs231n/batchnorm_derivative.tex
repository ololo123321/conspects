\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{derivative}
\usepackage{tikz}

\begin{document}

\title{Derivative of scalar function w.r.t. arguments of BatchNorm operation}
\author{Kadulin V.}
%\date{}
\maketitle

Let $Y = \gamma \hat{X} + \beta$, where 
$\hat{X} \in \mathbb{R}^{N \times D}$, 
$\gamma \in \mathbb{R}^D$, 
$\beta \in \mathbb{R}^D$, 
$\hat{X} = \frac{X - \mu}{\sigma}$,  
$\mu = \frac{1}{N} \sum_{i=1}^{N} x_{i}$,
$v = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2$,
$\sigma = \sqrt{v + \epsilon}$,
$\epsilon \in \mathbb{R}$ is a small scalar to omit zero division. 

Let $f: \mathbb{R}^{N \times D} \rightarrow \mathbb{R}$ is a differentiable scalar function. We also know $\pdv{f}{Y}$.
Derive $\pdv{f}{X}$, $\pdv{f}{\gamma}$, $\pdv{f}{\beta}$.

Notice that
\begin{itemize}
\item $y_i$ depends on $x_j$, where $1 \le j \le N$
\item there is only per-column dependency: $y_{ij}$ depends on $x_{kj}$, where $1 \le k \le N$
\item $\gamma_j$ and $\beta_{j}$ is the same for all ${y_{ij}}$, $1 \le i \le N$, $1 \le j \le D$.
\end{itemize}

Because columns are independent, consider $j$-th column of $X$ as $x$. Hence,

\begin{equation*}
\begin{split}
y = \gamma \hat{x} + \beta \\
y, x, \hat{x} \in \mathbb{R}^N \\
\gamma, \beta, \mu, v, \sigma \in \mathbb{R}
\end{split}
\end{equation*}

$\pdv{f}{x_j} = \sum_{i=1}^{N} \pdv{f}{y_i} \pdv{y_i}{\hat{x_i}} \pdv{\hat{x_i}}{x_j}$, where $x_j$ is the $j$-th element of $x$.

$
	\pdv{f}{\gamma} = 
	\sum_{i=1}^{N} \pdv{f}{y_i} \pdv{y_i}{\gamma} = 
	\sum_{i=1}^{N} \pdv{f}{y_i} \hat{x_i}
$

$
	\pdv{f}{\beta} = 
	\sum_{i=1}^{N} \pdv{f}{y_i} \pdv{y_i}{\beta} =
	\sum_{i=1}^{N} \pdv{f}{y_i}
$

Tricky part here is $\pdv{f}{x_j}$. Let's derive it with two approaches: forward-mode differentiation (deriving $\pdv{f}{x_j}$ directly from $f(x_j)$) and backward-mode differentiation (through writing computation graph and traversing it in reverse mode: from $f$ to $x_j$).

Knowing expression for $\pdv{f}{x_j}$, expression for $\pdv{f}{X}$ comes straightforwardly from it.


%\newpage

\section*{Forward-mode differentiation}  

Let's derive expressions for building blocks:

$\pdv{y}{\hat{x_i}} = \gamma$ 

$\pdv{\hat{x_i}}{x_j} = 
\pdv{(x_i - \mu)}{x_j} (v + \epsilon)^{-\frac{1}{2}} + (x_i - \mu) (-\frac{1}{2}) (v + \epsilon)^{-\frac{3}{2}} \pdv{(v + \epsilon)}{x_j}$

$\pdv{\mu}{x_j} = \frac{1}{N}$

$
\pdv{x_i}{x_j} = 
\begin{cases}
1 &\text{$i = j$}\\
0 &\text{$i \ne j$}
\end{cases}
$

Let's derive $\pdv{v}{x_j}$.

Consider $i$-th component of sum in $v$ as $y_i$.

$
\pdv{y_i}{x_j} = 
\begin{cases}
2 (x_i - \mu) (1 - \frac{1}{N}) = -\frac{2}{N} (x_i - \mu) + 2(x_i - \mu) & \text{$i = j$} \\
-\frac{2}{N} (x_i - \mu) & \text{$i \ne j$}
\end{cases}
$ 

So, $\pdv{v}{x_j} = \frac{1}{N} ( -\frac{2}{N} \sum_{i=1}^{N}(x_i - \mu) + 2(x_j - \mu))$.

Note, that $\sum_{i=1}^{N}(x_i - \mu) = \sum_{i=1}^{N} x_i - N \mu = N \mu - N \mu = 0$.

So, $\pdv{v}{x_j} = \frac{2}{N} (x_j - \mu)$.

Now we can expand $\pdv{\hat{x_i}}{x_j}$. Let's do it in case $i \ne j$:

\begin{gather*}
\begin{split}
\pdv{\hat{x_i}}{x_j} =
-\frac{1}{N} (v + \epsilon)^{-\frac{1}{2}} + (x_i - \mu) (-\frac{1}{2}) (v + \epsilon)^{-\frac{3}{2}} \frac{2}{N}(x_j - \mu) = \\
-\frac{1}{N} (v + \epsilon)^{-\frac{1}{2}} (1 + (x_i - \mu) (x_j - \mu) (v + \epsilon)^{-1})
\end{split}
\end{gather*}

Note that the only difference of case $i = j$ is the first multiplier of the first term: it's $1 - \frac{1}{N}$ instead of $-\frac{1}{N}$.

Now we have all to derive $\pdv{f}{x_j}$:

\begin{gather*}
\pdv{f}{x_j} = 
\sum_{i=1}^{N} \pdv{f}{y_i} \pdv{y_i}{\hat{x_i}} \pdv{\hat{x_i}}{x_j} = 
\gamma \sum_{i=1}^{N} \pdv{f}{y_i} \pdv{\hat{x_i}}{x_j} = \\
\gamma \left( \sum_{i=1}^{N} -\frac{1}{N} (v + \epsilon)^{-\frac{1}{2}} (1 + (x_i - \mu) (x_j - \mu) (v + \epsilon)^{-1}) \pdv{f}{y_i} + (v + \epsilon)^{-\frac{1}{2}} \pdv{f}{y_j} \right) = \\
\gamma (v + \epsilon)^{-\frac{1}{2}} \left( -\frac{1}{N} \sum_{i=1}^{N} (1 + (x_i - \mu) (x_j - \mu) (v + \epsilon)^{-1}) \pdv{f}{y_i} + \pdv{f}{y_j} \right) = \\
\gamma (v + \epsilon)^{-\frac{1}{2}} \left( -\frac{1}{N} \sum_{i=1}^{N} \pdv{f}{y_i} - (x_j - \mu) (v + \epsilon)^{-1} \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu) \pdv{f}{y_i} + \pdv{f}{y_j} \right)
\end{gather*}

%\newpage

\section*{Backward-mode differentiation}

Let's draw a computation graph:

\begin{tikzpicture}[node distance={25mm}, main/.style = {draw, circle}]
\node[main] (1) {$x_j$};
\node[main] (2) [above right of=1] {$\mu$};
\node[main] (3) [below right of=1] {$v$};
\node[main] (4) [below right of=2] {$\hat{x_i}$};
\node[main] (5) [below right of=4] {$\gamma$};
\node[main] (6) [above right of=5] {$y_i$};
\node[main] (7) [right of=6] {$f$};
\draw[->] (1) -- (2);
\draw[->] (1) -- (3);
\draw[->] (1) -- (4);
\draw[->] (2) -- (3);
\draw[->] (2) -- (4);
\draw[->] (3) -- (4);
\draw[->] (4) -- (6);
\draw[->] (5) -- (6);
\draw[->] (6) -- (7);
\end{tikzpicture}

Note that there are three paths from $x_j$ and two paths from $\mu$. So, derivative expressions through these nodes will consist of three and two terms respectively:

\[
	\pdv{f}{x_j} = 
	\sum_{i=1}^{N}
	\pdv{f}{y_i}
	\pdv{y_i}{\hat{x_i}} 
	\left( 
		\pdv{\hat{x_i}}{x_j} + 
		\pdv{\hat{x_i}}{v}\pdv{v}{x_j} + 
		\left( 
			\pdv{\hat{x_i}}{\mu} + 
			\pdv{\hat{x_i}}{v} \pdv{v}{\mu} 
		\right) 
		\pdv{\mu}{x_j} 
	\right) 
\]

$\pdv{y_i}{\hat{x_i}} = \gamma$

$
\pdv{\hat{x_i}}{x_j} = 
\begin{cases}
1 & \text{$i = j$} \\
0 & \text{$i \ne j$}
\end{cases}
$

$\pdv{\hat{x_i}}{v} = (x_i - \mu) (- \frac{1}{2}) (v + \epsilon)^{-\frac{3}{2}}$

$\pdv{v}{x_j} = \frac{2}{N} (x_j - \mu)$

$\pdv{\hat{x_i}}{\mu} = - (v + \epsilon)^{-\frac{1}{2}}$

$\pdv{v}{\mu} = - \frac{2}{N} \sum_{i=1}^{N} (x_i - \mu) = 0$

$\pdv{\mu}{x_j} = \frac{1}{N}$

Now let's derive $\pdv{f}{x_j}$:

\begin{gather*}
\pdv{f}{x_j} = 
	\gamma 
	\left(
		\sum_{i=1}^{N} \pdv{f}{y_i} 
			\left( 
				- (x_i - \mu) (v + \epsilon)^{-\frac{3}{2}} \frac{1}{N} (x_j - \mu) - \frac{1}{N} (v + \epsilon)^{-\frac{1}{2}}
			\right) 
			+ (v + \epsilon)^{-\frac{1}{2}} \pdv{f}{y_j}
	\right) = \\
	\gamma (v + \epsilon)^{-\frac{1}{2}} 
	\left(	
		- \frac{1}{N} \sum_{i=1}^{N} \pdv{f}{y_i}
		- (x_j - \mu) (v + \epsilon)^{-1} \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu) \pdv{f}{y_i} 
		+ \pdv{f}{y_j}
	\right)
\end{gather*}

Note that result is the same as in previous approach.

Knowing $\pdv{f}{x_j}$, it's easy to write $\pdv{f}{X}$:

\[
	\pdv{f}{X} = \gamma (v + \epsilon)^{-\frac{1}{2}} \left( -\frac{1}{N} \sum_{i=1}^{N} \pdv{f}{y_i} - (X - \mu) (v + \epsilon)^{-1} \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu) \pdv{f}{y_i} + \pdv{f}{Y} \right) 
\]

Here $x, y, v, \mu, \gamma \in \mathbb{R}^D$.

\end{document}